"Application.py"

```
# Application.py

import streamlit as st
import pandas as pd
from Binning_Tab.data_binner import DataBinner
from Binning_Tab.data_integrity_assessor import DataIntegrityAssessor
from Binning_Tab.unique_bin_identifier import UniqueBinIdentifier
import os
import traceback  # For detailed error logging

# Import utility functions and directory constants
from Binning_Tab.utils import (
    hide_streamlit_style,
    load_data,
    align_dataframes,
    save_dataframe,
    run_processing,
    PLOTS_DIR,
    PROCESSED_DATA_DIR,
    REPORTS_DIR,
    UNIQUE_IDENTIFICATIONS_DIR,
    get_binning_configuration,
    plot_entropy_and_display,
    plot_density_plots_and_display,
    handle_download_binned_data,
    handle_integrity_assessment,
    handle_unique_identification_analysis,
    display_unique_identification_results
)

def setup_page():
    """Configure the Streamlit page and apply custom styles."""
    st.set_page_config(
        page_title="🛠️ Data Processing and Binning Application",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    hide_streamlit_style()
    st.title('🛠️ Data Processing and Binning Application')

def sidebar_inputs():
    """Render the sidebar with file upload, settings, binning options, and info."""
    with st.sidebar:
        st.header("📂 Upload & Settings")
        uploaded_file = st.file_uploader("📤 Upload your dataset", type=['csv', 'pkl'])
        output_file_type = st.selectbox('📁 Select Output File Type', ['csv', 'pkl'], index=0)
        st.markdown("---")

        # Display warning if CSV is selected
        if output_file_type == 'csv':
            st.warning("⚠️ **Note:** Using CSV will result in the loss of some meta-data regarding data types. This will not affect the application's functionality.")

        st.header("⚙️ Binning Options")
        binning_method = st.selectbox('🔧 Select Binning Method', ['Quantile', 'Equal Width'])
        if binning_method == 'Equal Width':
            st.warning("⚠️ **Note:** Using Equal Width will drastically affect the distribution of your data. (Large integrity loss)")  
        
        st.markdown("---")

        st.header("ℹ️ About")
        st.info("""
            This application allows you to upload a dataset, process and bin numerical and datetime columns, 
            assess data integrity post-binning, visualize data distributions, and perform unique identification analysis.
        """)

    return uploaded_file, output_file_type, binning_method

def load_and_preview_data(uploaded_file, input_file_type):
    """Load the uploaded data and display a preview."""
    try:
        with st.spinner('Loading data...'):
            Data, error = load_data(input_file_type, uploaded_file)
        if error:
            st.error(error)
            st.stop()
    except Exception as e:
        st.error(f"Error loading data: {e}")
        st.error(traceback.format_exc())
        st.stop()

    # Clean column names by removing '\' and '/'
    Data.columns = Data.columns.str.replace(r'[\\/]', '', regex=True)

    st.session_state.Original_Data = Data.copy()
    st.session_state.Global_Data = Data.copy()  # Initialize Global_Data

    st.subheader('📊 Data Preview (Original Data)')
    st.dataframe(st.session_state.Original_Data.head())

def save_raw_data(Data, output_file_type):
    """Save the raw data to a CSV or Pickle file."""
    mapped_save_type = 'pickle' if output_file_type == 'pkl' else 'csv'
    data_csv_path = f'Data.{output_file_type}'
    try:
        if mapped_save_type == 'pickle':
            Data.to_pickle(data_csv_path)
        else:
            Data.to_csv(data_csv_path, index=False)
    except Exception as e:
        st.error(f"Error saving Data.{output_file_type}: {e}")
        st.stop()
    return mapped_save_type, data_csv_path

def run_data_processing(mapped_save_type, output_file_type, data_csv_path):
    """Run the data processing pipeline."""
    processed_data = run_processing(
        save_type=mapped_save_type,
        output_filename=f'processed_data.{output_file_type}',
        file_path=data_csv_path
    )

    st.session_state.Processed_Data = processed_data.copy()

def initialize_session_state():
    """Initialize session state variables if not already present."""
    if 'Original_Data' not in st.session_state:
        st.session_state.Original_Data = pd.DataFrame()
    if 'Global_Data' not in st.session_state:
        st.session_state.Global_Data = pd.DataFrame()
    if 'Binning_Selected_Columns' not in st.session_state:
        st.session_state.Binning_Selected_Columns = []
    # Removed 'Manipulation_Selected_Columns' as it's no longer needed

def main():
    """Main function to orchestrate the Streamlit app."""
    setup_page()
    initialize_session_state()
    uploaded_file, output_file_type, binning_method = sidebar_inputs()

    if uploaded_file is not None:
        # Determine input file type
        if uploaded_file.name.endswith('.csv'):
            input_file_type = 'csv'
        elif uploaded_file.name.endswith('.pkl'):
            input_file_type = 'pkl'
        else:
            st.error("Unsupported file type! Please upload a CSV or Pickle (.pkl) file.")
            st.stop()

        load_and_preview_data(uploaded_file, input_file_type)
        mapped_save_type, data_csv_path = save_raw_data(st.session_state.Original_Data, output_file_type)
        run_data_processing(mapped_save_type, output_file_type, data_csv_path)
    else:
        st.info("🔄 **Please upload a file to get started.**")
        st.stop()

    # Create Tabs
    tabs = st.tabs(["📊 Binning", "🛠️ Manipulation", "🔍 Unique Identification Analysis"])

    ######################
    # Binning Tab
    ######################
    with tabs[0]:
        st.header("📊 Binning")
        st.markdown("### 🔢 Select Columns to Bin")

        # Determine columns available for binning (no exclusion)
        available_columns = st.session_state.Processed_Data.select_dtypes(
            include=['number', 'datetime', 'datetime64[ns, UTC]', 'datetime64[ns]']
        ).columns.tolist()

        selected_columns = st.multiselect(
            'Select columns to bin',
            options=available_columns,
            default=st.session_state.Binning_Selected_Columns,
            key='binning_columns'
        )
        st.session_state.Binning_Selected_Columns = selected_columns

        if selected_columns:
            Data_aligned, binned_df_aligned = perform_binning(
                st.session_state.Processed_Data,
                selected_columns,
                binning_method
            )
            perform_integrity_assessment(Data_aligned, binned_df_aligned, selected_columns)
            plot_density(
                Data_aligned[selected_columns].astype('category'),
                binned_df_aligned[selected_columns],
                selected_columns
            )
            # Update Global_Data
            st.session_state.Global_Data = st.session_state.Original_Data.copy()
            for col in selected_columns:
                st.session_state.Global_Data[col] = binned_df_aligned[col]

            st.subheader('📊 Data Preview (Global Data)')
            st.dataframe(st.session_state.Global_Data.head())

            download_binned_data()
        else:
            st.info("🔄 **Please select at least one column to bin.**")

    ######################
    # Manipulation Tab (Placeholder)
    ######################
    with tabs[1]:
        st.header("🛠️ Manipulation")
        st.markdown("### 🔧 Adding this feature later.")

        st.info("🔄 **Manipulation functionality will be added here in future updates.**")

    ######################
    # Unique Identification Analysis Tab
    ######################
    with tabs[2]:
        st.header("🔍 Unique Identification Analysis")
        st.markdown("### 🔢 Selected Columns for Analysis")

        # Use only columns selected in Binning tab
        selected_columns = st.session_state.Binning_Selected_Columns

        if not selected_columns:
            st.warning("⚠️ **No columns selected in Binning tab for analysis.**")
            st.info("🔄 **Please select columns in the Binning tab to perform Unique Identification Analysis.**")
        else:
            st.write(f"**Columns selected for analysis:** {', '.join(selected_columns)}")

            # Format selected columns as categorical
            analysis_df = st.session_state.Global_Data[selected_columns].astype('category')

            unique_identification_section(
                original_for_assessment=st.session_state.Original_Data[selected_columns].astype('category'),
                binned_for_assessment=analysis_df,
                selected_columns=selected_columns
            )

def perform_binning(processed_data, selected_columns, binning_method):
    """Perform the binning process on selected columns."""
    try:
        bins = get_binning_configuration(processed_data, selected_columns)
    except Exception as e:
        st.error(f"Error in binning configuration: {e}")
        st.stop()

    st.markdown("### 🔄 Binning Process")
    try:
        with st.spinner('Binning data...'):
            binner = DataBinner(processed_data, method=binning_method.lower())
            binned_df, binned_columns = binner.bin_columns(bins)

            # Align both DataFrames (original and binned) to have the same columns
            Data_aligned, binned_df_aligned = align_dataframes(processed_data, binned_df)
    except Exception as e:
        st.error(f"Error during binning: {e}")
        st.error(traceback.format_exc())
        st.stop()

    st.success("✅ Binning completed successfully!")

    # Display binned columns categorization
    st.markdown("### 🗂️ Binned Columns Categorization")
    for dtype, cols in binned_columns.items():
        if cols:
            st.write(f"  - **{dtype.capitalize()}**: {', '.join(cols)}")

    return Data_aligned, binned_df_aligned

def perform_integrity_assessment(Data_aligned, binned_df_aligned, selected_columns):
    """Assess data integrity after binning."""
    original_for_assessment = Data_aligned[selected_columns].astype('category')
    binned_for_assessment = binned_df_aligned[selected_columns]

    handle_integrity_assessment(original_for_assessment, binned_for_assessment, PLOTS_DIR)

def plot_density(original_for_assessment, binned_for_assessment, selected_columns):
    """Plot and display density plots."""
    plot_density_plots_and_display(original_for_assessment, binned_for_assessment, selected_columns, PLOTS_DIR)

def download_binned_data():
    """Handle downloading of the binned data."""
    handle_download_binned_data(
        data=st.session_state.Global_Data,
        file_type_download=st.selectbox('📁 Select Download File Type', ['csv', 'pkl'], index=0, key='download_file_type_download'),
        save_dataframe_func=save_dataframe,
        plots_dir=PLOTS_DIR
    )

def unique_identification_section(original_for_assessment, binned_for_assessment, selected_columns):
    """Handle the Unique Identification Analysis section."""
    st.markdown("### 🔍 Unique Identification Analysis")
    with st.expander("ℹ️ **About:**"):
        st.write("""
            This section analyzes combinations of binned columns to determine how many unique observations
            in the original dataset can be uniquely identified by each combination of bins.
            It helps in understanding the discriminative power of your binned features.
        """)

    # Use a form to group inputs and button together
    with st.form("unique_id_form"):
        st.write("#### 🧮 Configure Unique Identification Analysis")
        # Define bin columns to consider (use selected columns)

        col_count = len(selected_columns)
        col1, col2 = st.columns(2)
        with col1:
            min_comb_size = st.number_input('Minimum Combination Size', min_value=1, max_value=col_count, value=1, step=1)
        with col2:
            max_comb_size = st.number_input('Maximum Combination Size', min_value=min_comb_size, max_value=col_count, value=col_count, step=1)

        if max_comb_size > 5:
            st.warning("⚠️  **Note:** Combinations larger than 5 may take a long time to compute depending on bin count.")

        # Submit button
        submit_button = st.form_submit_button(label='🧮 Perform Unique Identification Analysis')

    if submit_button:
        results = handle_unique_identification_analysis(
            original_df=original_for_assessment,
            binned_df=binned_for_assessment,
            bin_columns_list=selected_columns,
            min_comb_size=min_comb_size,
            max_comb_size=max_comb_size
        )
        display_unique_identification_results(results)

if __name__ == "__main__":
    main()

```

"extract.py"

```
# extract.py

import os
import argparse

def parse_arguments():
    """
    Parses command-line arguments.
    """
    parser = argparse.ArgumentParser(
        description='Search for files with specific formats and write their contents to a text file.'
    )

    parser.add_argument(
        '-d', '--directory',
        type=str,
        default=r"C:\Users\matth\OneDrive\Desktop\1. DATA SCIENCE MASTER\Capstone_CITS5553\Application",
        help='The directory to search. Default is "C:\\Users\\matth\\OneDrive\\Desktop\\1. DATA SCIENCE MASTER\\Research_CITS5014\\MY_OWN_GAT"'
    )

    parser.add_argument(
        '-f', '--formats',
        type=str,
        nargs='+',
        default=[".py"],
        help='One or more file formats to search for (e.g., .py .txt .ipynb). Default is ".py"'
    )

    parser.add_argument(
        '-i', '--ignore',
        type=str,
        nargs='*',
        default=[],
        help='One or more directories to ignore. Default is to always ignore ".venv"'
    )

    return parser.parse_args()

def main():
    """
    Main function to execute the script logic.
    """
    args = parse_arguments()
    search_dir = args.directory
    file_formats = set(args.formats)
    
    # Always ignore '.venv' plus any additional directories specified
    ignore_dirs = set(args.ignore) | {'.venv'}

    # Path to the output file
    output_file = os.path.join(search_dir, "FileName_Contents.txt")

    # Clear the output file if it exists
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            pass  # This will clear the file
    except Exception as e:
        print(f"Error initializing the output file: {e}")
        return

    # Walk through the directory
    for root, dirs, files in os.walk(search_dir, topdown=True):
        # Modify dirs in-place to skip ignored directories
        dirs[:] = [d for d in dirs if d not in ignore_dirs]

        for file in files:
            if any(file.endswith(ext) for ext in file_formats):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except Exception as e:
                    print(f"Error reading {file_path}: {e}")
                    continue

                # Write the file name and its contents to the output file
                try:
                    with open(output_file, 'a', encoding='utf-8') as out_f:
                        out_f.write(f'"{file}"\n\n```\n{content}\n```\n\n')
                except Exception as e:
                    print(f"Error writing to {output_file}: {e}")
                    return

    print(f"Contents have been successfully written to {output_file}")

if __name__ == "__main__":
    main()

```

"data_binner.py"

```
# data_binner.py

import pandas as pd
from typing import Tuple, Dict, List

class DataBinner:
    """
    A class to bin specified columns in a Pandas DataFrame based on provided bin counts and binning methods.
    """
    
    def __init__(self, Data: pd.DataFrame, method: str = 'equal width'):
        """
        Initializes the DataBinner with the original DataFrame and binning method.
        """
        self.original_df = Data.copy()
        self.binned_df = pd.DataFrame()
        self.binned_columns = {
            'datetime': [],
            'integer': [],
            'float': [],
            'unsupported': []
        }
        self.method = method.lower()
        self._validate_method()
    
    def _validate_method(self):
        """
        Validates the binning method. Raises a ValueError if the method is unsupported.
        """
        supported_methods = ['equal width', 'quantile']
        if self.method not in supported_methods:
            raise ValueError(f"Unsupported binning method '{self.method}'. Supported methods are: {supported_methods}")
    
    def bin_columns(
        self,
        bin_dict: Dict[str, int]
    ) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:
        """
        Bins specified columns in the DataFrame based on the provided bin counts and binning method.
        """
        # Initialize dictionary to categorize binned columns
        self.binned_columns = {
            'datetime': [],
            'integer': [],
            'float': [],
            'unsupported': []
        }

        # Create a copy of the DataFrame to avoid modifying the original data
        Bin_Data = self.original_df.copy()

        for col, bins in bin_dict.items():
            if col not in Bin_Data.columns:
                print(f"⚠️ Column '{col}' does not exist in the DataFrame. Skipping.")
                continue

            try:
                if pd.api.types.is_datetime64_any_dtype(Bin_Data[col]):
                    # Binning datetime columns using pd.cut or pd.qcut based on method
                    Bin_Data[col] = self._bin_column(Bin_Data[col], bins, self.method)
                    self.binned_columns['datetime'].append(col)

                elif pd.api.types.is_integer_dtype(Bin_Data[col]):
                    Bin_Data[col] = self._bin_column(Bin_Data[col], bins, self.method)
                    self.binned_columns['integer'].append(col)

                elif pd.api.types.is_float_dtype(Bin_Data[col]):
                    Bin_Data[col] = self._bin_column(Bin_Data[col], bins, self.method)
                    self.binned_columns['float'].append(col)

                else:
                    print(f"Column '{col}' has unsupported dtype '{Bin_Data[col].dtype}'. Skipping.")
                    self.binned_columns['unsupported'].append(col)

            except Exception as e:
                # Detailed error messages based on column type
                if pd.api.types.is_datetime64_any_dtype(Bin_Data[col]):
                    print(f"Failed to bin datetime column '{col}': {e}")
                elif pd.api.types.is_integer_dtype(Bin_Data[col]):
                    print(f"Failed to bin integer column '{col}': {e}")
                elif pd.api.types.is_float_dtype(Bin_Data[col]):
                    print(f"Failed to bin float column '{col}': {e}")
                else:
                    print(f"Failed to bin column '{col}': {e}")
                self.binned_columns['unsupported'].append(col)

        # Retain only the successfully binned columns
        successfully_binned = (
            self.binned_columns['datetime'] +
            self.binned_columns['integer'] +
            self.binned_columns['float']
        )
        self.binned_df = Bin_Data[successfully_binned]

        return self.binned_df, self.binned_columns

    def _bin_column(self, series: pd.Series, bins: int, method: str) -> pd.Series:
        """
        Bins a single column using the specified method and returns integer labels as categorical.

        Parameters:
            series (pd.Series): The column to bin.
            bins (int): The number of bins.
            method (str): The binning method ('equal width' or 'quantile').

        Returns:
            pd.Series: The binned column as categorical integers starting at 1.
        """
        if method == 'equal width':
            binned = pd.cut(
                series,
                bins=bins,
                labels=False,
                duplicates='drop'
            )
        elif method == 'quantile':
            binned = pd.qcut(
                series,
                q=bins,
                labels=False,
                duplicates='drop'
            )
        else:
            # This should not happen due to validation in __init__
            raise ValueError(f"Unsupported binning method '{method}'.")

        # If labels=False, bins start at 0. To start at 1, add 1 and convert to categorical
        return (binned + 1).astype('category')

    def get_binned_data(self) -> pd.DataFrame:
        """
        Retrieves the binned DataFrame.
        """
        return self.binned_df.copy()

    def get_binned_columns(self) -> Dict[str, List[str]]:
        """
        Retrieves the categorization of binned columns by data type.
        """
        return self.binned_columns.copy()

```

"data_integrity_assessor.py"

```
# data_integrity_assessor.py

import pandas as pd
import numpy as np
from scipy.stats import entropy
import matplotlib.pyplot as plt
import os

class DataIntegrityAssessor:
    def __init__(self, original_df: pd.DataFrame, binned_df: pd.DataFrame):
        self.original_df = original_df.copy()
        self.binned_df = binned_df.copy()
        self.integrity_report = None
        self.overall_loss = None

        self._validate_dataframes()

    def _validate_dataframes(self):
        if not self.original_df.columns.equals(self.binned_df.columns):
            raise ValueError("Both DataFrames must have the same columns.")

        for col in self.original_df.columns:
            if not pd.api.types.is_object_dtype(self.original_df[col]) and not pd.api.types.is_categorical_dtype(self.original_df[col]):
                raise TypeError(f"Column '{col}' is not categorical in the original DataFrame.")
            if not pd.api.types.is_object_dtype(self.binned_df[col]) and not pd.api.types.is_categorical_dtype(self.binned_df[col]):
                raise TypeError(f"Column '{col}' is not categorical in the binned DataFrame.")

    @staticmethod
    def calculate_entropy(series: pd.Series) -> float:
        counts = series.value_counts(normalize=True)
        return entropy(counts, base=2)

    def assess_integrity_loss(self):
        integrity_data = {
            'Variable': [],
            'Original Entropy (bits)': [],
            'Binned Entropy (bits)': [],
            'Entropy Loss (bits)': [],
            'Percentage Loss (%)': []
        }

        for col in self.original_df.columns:
            original_entropy = self.calculate_entropy(self.original_df[col])
            binned_entropy = self.calculate_entropy(self.binned_df[col])
            entropy_loss = original_entropy - binned_entropy
            percentage_loss = (entropy_loss / original_entropy) * 100 if original_entropy != 0 else 0

            integrity_data['Variable'].append(col)
            integrity_data['Original Entropy (bits)'].append(round(original_entropy, 6))
            integrity_data['Binned Entropy (bits)'].append(round(binned_entropy, 6))
            integrity_data['Entropy Loss (bits)'].append(round(entropy_loss, 6))
            integrity_data['Percentage Loss (%)'].append(round(percentage_loss, 2))

        self.integrity_report = pd.DataFrame(integrity_data)
        self.overall_loss = round(self.integrity_report['Percentage Loss (%)'].mean(), 2)

    def generate_report(self) -> pd.DataFrame:
        if self.integrity_report is None:
            self.assess_integrity_loss()
        return self.integrity_report.copy()

    def save_report(self, filepath: str):
        if self.integrity_report is None:
            self.assess_integrity_loss()
        self.integrity_report.to_csv(filepath, index=False)
        print(f"Integrity report saved to {os.path.abspath(filepath)}")

    def plot_entropy(self, save_path: str = None, figsize: tuple = (10, 6)):
        if self.integrity_report is None:
            self.assess_integrity_loss()

        variables = self.integrity_report['Variable']
        original_entropy = self.integrity_report['Original Entropy (bits)']
        binned_entropy = self.integrity_report['Binned Entropy (bits)']

        x = np.arange(len(variables))  # the label locations
        width = 0.35  # the width of the bars

        fig, ax = plt.subplots(figsize=figsize)
        rects1 = ax.bar(x - width/2, original_entropy, width, label='Original Entropy', alpha=0.5, edgecolor='blue', color='blue')
        rects2 = ax.bar(x + width/2, binned_entropy, width, label='Binned Entropy', alpha=0.5, edgecolor='orange', color='orange')

        # Add some text for labels, title and custom x-axis tick labels, etc.
        ax.set_ylabel('Entropy (bits)')
        ax.set_title('Original vs Binned Entropy per Variable')
        ax.set_xticks(x)
        ax.set_xticklabels(variables, rotation=45, ha='right')
        ax.legend()

        # Attach a text label above each bar in rects, displaying its height.
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height:.2f}',
                            xy=(rect.get_x() + rect.get_width() / 2, height),
                            xytext=(0, 3),  # 3 points vertical offset
                            textcoords="offset points",
                            ha='center', va='bottom')

        autolabel(rects1)
        autolabel(rects2)

        fig.tight_layout()

        if save_path:
            plt.savefig(save_path, bbox_inches='tight')
            print(f"Entropy plot saved to {os.path.abspath(save_path)}")
        else:
            plt.show()
    
        return fig  # Add this line to return the Figure object

    def get_overall_loss(self) -> float:
        if self.overall_loss is None:
            self.assess_integrity_loss()
        return self.overall_loss

```

"density_plotter.py"

```
# density_plotter.py

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Optional, Tuple
import math
import os

class DensityPlotter:
    """
    A class to generate density plots for selected categorical or integer columns in a Pandas DataFrame.

    Supported Data Types:
        - Categorical
        - Integer (treated as categorical)
    """

    def __init__(
        self,
        dataframe: pd.DataFrame,
        category_columns: List[str],
        figsize: Tuple[int, int] = (20, 20),
        save_path: Optional[str] = None,
        plot_style: str = 'whitegrid'
    ):
        """
        Initialize the DensityPlotter with a DataFrame and selected columns.

        Parameters:
            dataframe (pd.DataFrame): The input DataFrame to plot.
            category_columns (List[str]): List of categorical or integer columns to plot.
            figsize (Tuple[int, int], optional): Size of the overall figure. Default is (20, 20).
            save_path (Optional[str], optional): Path to save the plot image. If None, the plot is displayed.
            plot_style (str, optional): Seaborn style for the plots. Default is 'whitegrid'.
        """
        self.dataframe = dataframe.copy()
        self.category_columns = category_columns
        self.figsize = figsize
        self.save_path = save_path
        self.plot_style = plot_style

        # Initialize Seaborn style
        sns.set_style(self.plot_style)

        # Validate columns
        self._validate_category_columns()

    def _validate_category_columns(self):
        """
        Validate that the specified columns exist in the DataFrame and have appropriate data types.
        """
        for col in self.category_columns:
            if col not in self.dataframe.columns:
                raise ValueError(f"Column '{col}' does not exist in the DataFrame.")
            if not (
                pd.api.types.is_categorical_dtype(self.dataframe[col]) or
                pd.api.types.is_integer_dtype(self.dataframe[col])
            ):
                raise TypeError(
                    f"Column '{col}' is not of categorical dtype or integer dtype."
                )

    def plot_grid(self):
        """
        Generate and display/save the grid of density plots for the selected columns.
        """
        total_plots = len(self.category_columns)
        if total_plots == 0:
            print("No columns to plot.")
            return

        # Determine grid size (rows and columns)
        cols = math.ceil(math.sqrt(total_plots))
        rows = math.ceil(total_plots / cols)

        # Create subplots
        fig, axes = plt.subplots(rows, cols, figsize=self.figsize)
        if total_plots == 1:
            axes = [axes]  # Ensure axes is iterable
        else:
            axes = axes.flatten()  # Flatten in case of multiple rows

        plot_idx = 0  # Index to track the current subplot

        # Plot Columns with Histogram and Density Plots
        for col in self.category_columns:
            ax = axes[plot_idx]
            try:
                # Convert integer columns to categorical codes
                if pd.api.types.is_integer_dtype(self.dataframe[col]):
                    data = self.dataframe[col].astype('category').cat.codes
                else:
                    data = self.dataframe[col].astype('category').cat.codes

                # Get the counts for each category
                counts = self.dataframe[col].value_counts().sort_index()

                # Determine if histogram should be overlaid
                unique_categories = counts.size

                # Plot histogram first if unique categories < 30
                if unique_categories < 100:
                    sns.histplot(
                        counts.values,
                        ax=ax,
                        color='blue',
                        alpha=0.4,
                        bins=unique_categories,
                        discrete=True,
                        stat='density',  # Normalize histogram to density
                        label='Histogram'
                    )

                    # Overlay density plot
                    sns.kdeplot(
                        data=counts.values,
                        ax=ax,
                        fill=True,
                        color='orange',
                        bw_adjust=0.5,  # Adjust bandwidth for better visualization
                        label='Density'
                    )

                    # Add legend to differentiate plots
                    ax.legend()
                else:
                    # Plot only density plot for columns with >=30 unique categories
                    sns.kdeplot(
                        data=counts.values,
                        ax=ax,
                        fill=True,
                        color='orange',
                        bw_adjust=0.5,  # Adjust bandwidth for better visualization
                        label='Density'
                    )
                    ax.legend()

                # Set plot titles and labels
                ax.set_title(col)             # Add title
                ax.set_ylabel('Density')
                ax.set_xlabel('')             # Remove x label
                ax.set_xticks([])             # Remove x ticks

            except Exception as e:
                print(f"Failed to plot column '{col}': {e}")
            plot_idx += 1

        # Remove any unused subplots
        for idx in range(plot_idx, len(axes)):
            fig.delaxes(axes[idx])

        plt.tight_layout()

        # Save or show the plot
        if self.save_path:
            # Ensure the directory exists
            save_dir = os.path.dirname(self.save_path)
            if save_dir:
                os.makedirs(save_dir, exist_ok=True)
            try:
                plt.savefig(self.save_path, dpi=300)
                print(f"Density plots saved to {self.save_path}")
            except Exception as e:
                print(f"Failed to save plot to '{self.save_path}': {e}")
        else:
            plt.show()

        return fig

```

"Detect_Dtypes.py"

```
# Detect_Dtypes.py

import pandas as pd
import numpy as np
import logging
from typing import Dict, Tuple, Optional
import sys
import concurrent.futures
import re


class DtypeDetector:
    def __init__(
        self,
        date_threshold: float = 0.5,
        numeric_threshold: float = 0.9,
        factor_threshold_ratio: float = 0.5,
        factor_threshold_unique: int = 50,
        dayfirst: bool = False,
        log_level: str = 'INFO',
        log_file: Optional[str] = None,
        convert_factors_to_int: bool = True,
        date_format: Optional[str] = None  # **New Parameter**
    ):
        """
        Initialize the DtypeDetector with configurable thresholds and logging.

        Parameters:
            date_threshold (float): Threshold for date detection.
            numeric_threshold (float): Threshold for numeric detection.
            factor_threshold_ratio (float): Threshold ratio for factor detection.
            factor_threshold_unique (int): Threshold for unique values in factor detection.
            dayfirst (bool): Whether to interpret the first value in dates as the day.
            log_level (str): Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
            log_file (Optional[str]): Path to save the log file. If None, logs are printed to stdout.
            convert_factors_to_int (bool): Whether to convert factors to integer codes. 
                If False, factors remain as categorical types with original string labels.
            date_format (Optional[str]): Desired date format (e.g., '%d-%m-%Y'). 
                If specified, date columns will be formatted as strings in this format.
        """
        self.convert_factors_to_int = convert_factors_to_int
        self.date_format = date_format  # **Store the New Parameter**
        self.thresholds = {
            'date_threshold': date_threshold,
            'numeric_threshold': numeric_threshold,
            'factor_threshold_ratio': factor_threshold_ratio,
            'factor_threshold_unique': factor_threshold_unique
        }
        self.dayfirst = dayfirst
        self.data_types: Dict[str, str] = {}
        self.series_mapping: Dict[str, Dict[int, str]] = {}

        # Configure logging
        log_handlers = [logging.StreamHandler(sys.stdout)]
        if log_file:
            log_handlers.append(logging.FileHandler(log_file))
        
        logging.basicConfig(
            level=getattr(logging, log_level.upper(), logging.INFO),
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=log_handlers
        )
        self.logger = logging.getLogger(__name__)
    
    def clean_column_names(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Clean column names by removing any '/' or '\' characters.

        Parameters:
            data (pd.DataFrame): The DataFrame with original column names.

        Returns:
            pd.DataFrame: The DataFrame with cleaned column names.
        """
        original_columns = data.columns.tolist()
        cleaned_columns = []
        for col in original_columns:
            # Remove '/' and '\' from column names
            cleaned_col = re.sub(r'[\\/]', '', col)
            cleaned_columns.append(cleaned_col)
            if cleaned_col != col:
                self.logger.debug(f"Renamed column '{col}' to '{cleaned_col}'")
        data.columns = cleaned_columns
        return data

    def determine_column_type(
        self,
        series: pd.Series
    ) -> str:
        """
        Determine the type of a pandas Series.
        Returns 'int', 'float', 'date', 'factor', 'bool', or 'string'.
        """
        total = len(series)
        if total == 0:
            self.logger.debug(f"Column '{series.name}' is empty. Defaulting to 'string'.")
            return 'string'  # Default to string for empty columns

        num_unique = series.nunique(dropna=True)
        self.logger.debug(f"Column '{series.name}': Total={total}, Unique={num_unique}")

        # Attempt to convert to numeric first
        try:
            s_numeric = pd.to_numeric(series, errors='coerce')
            num_not_missing_numeric = s_numeric.notnull().sum()
            percent_numeric = num_not_missing_numeric / total
            self.logger.debug(f"Column '{series.name}': Numeric parse success rate: {percent_numeric:.2f}")
            if percent_numeric > self.thresholds['numeric_threshold']:
                # Check if all non-NaN values are integers within a tolerance
                if np.allclose(s_numeric.dropna(), s_numeric.dropna().astype(int), atol=1e-8):
                    return 'int'
                else:
                    return 'float'
        except Exception as e:
            self.logger.debug(f"Column '{series.name}': Numeric parsing failed: {e}")

        # Attempt to parse dates
        try:
            # Include time in date parsing by specifying formats that include time
            # Example formats: '%d/%m/%Y %H:%M', '%Y-%m-%d %H:%M', etc.
            # You can expand this list based on your data
            date_formats = [
                '%d/%m/%Y %H:%M',
                '%d/%m/%Y',
                '%Y-%m-%d %H:%M',
                '%Y-%m-%d',
                '%m/%d/%Y %H:%M',
                '%m/%d/%Y',
                '%d-%m-%Y %H:%M',
                '%d-%m-%Y',
                '%Y/%m/%d %H:%M',
                '%Y/%m/%d',
                '%Y-%m-%d %H:%M:%S%z',
                '%a %b %d %H:%M:%S %z %Y',
                '%a %b %d %H:%M:%S +0000 %Y'
            ]
            for fmt in date_formats:
                s_date = pd.to_datetime(series, errors='coerce', format=fmt, dayfirst=self.dayfirst)
                num_not_missing_date = s_date.notnull().sum()
                percent_date = num_not_missing_date / total
                self.logger.debug(f"Column '{series.name}': Date parse success rate with format '{fmt}': {percent_date:.2f}")
                if percent_date > self.thresholds['date_threshold']:
                    return 'date'
        except Exception as e:
            self.logger.debug(f"Column '{series.name}': Date parsing failed: {e}")

        # Check for boolean
        unique_values = set(series.dropna().unique())
        if unique_values <= {0, 1, '0', '1', 'True', 'False', 'true', 'false'}:
            return 'bool'

        # Check for categorical (factor) with AND condition
        try:
            if (num_unique / total) < self.thresholds['factor_threshold_ratio'] and num_unique < self.thresholds['factor_threshold_unique']:
                return 'factor'
        except Exception as e:
            self.logger.debug(f"Column '{series.name}': Factor determination failed: {e}")

        return 'string'

    def convert_series(self, series: pd.Series, dtype: str) -> pd.Series:
        """
        Convert a pandas Series to the specified dtype.
        """
        if dtype == 'date':
            dt_series = pd.to_datetime(series, errors='coerce', dayfirst=self.dayfirst)
            if self.date_format:
                # Format datetime as string in the specified format
                formatted_series = dt_series.dt.strftime(self.date_format)
                return formatted_series
            else:
                return dt_series
        elif dtype == 'factor':
            category = series.astype('category')
            # Store the mapping of codes to categories
            self.series_mapping[series.name] = dict(enumerate(category.cat.categories))
            if self.convert_factors_to_int:
                return category.cat.codes  # **Convert to integer codes**
            else:
                return category  # **Retain as categorical with string labels**
        elif dtype == 'int':
            return pd.to_numeric(series, errors='coerce').astype('Int64')
        elif dtype == 'float':
            return pd.to_numeric(series, errors='coerce')
        elif dtype == 'bool':
            # Map various representations of booleans to actual booleans
            return series.map({
                'True': True, 'False': False, 'true': True, 'false': False,
                1: True, 0: False
            }).astype('bool')
        else:
            return series.astype(str)

    def process_column(self, col: str, data: pd.DataFrame) -> Tuple[str, str, pd.Series]:
        """
        Process a single column: determine its type and convert it.
        Returns the column name, detected type, and converted series.
        """
        try:
            dtype = self.determine_column_type(data[col])
            converted_series = self.convert_series(data[col], dtype)
            self.logger.info(f"Column: {col}, Type Assessed: {dtype}, New Type: {converted_series.dtype}")
            return (col, dtype, converted_series)
        except Exception as e:
            self.logger.warning(f"Failed to process column '{col}': {e}")
            # Default to string if conversion fails
            converted_series = data[col].astype(str)
            self.logger.info(f"      New Type: {converted_series.dtype} (defaulted to string)")
            return (col, 'string', converted_series)

    def process_dataframe(
        self,
        filepath: str,
        use_parallel: bool = True,
        report_path: str = 'Type_Conversion_Report.csv'
    ) -> pd.DataFrame:
        """
        Read a CSV file, determine column types, convert columns accordingly, and generate a report.

        Parameters:
            filepath (str): Path to the input CSV file.
            use_parallel (bool): Whether to use parallel processing for columns.
            report_path (str): Path to save the type conversion report.

        Returns:
            pd.DataFrame: The processed DataFrame.
        """
        try:
            data = pd.read_csv(filepath, sep=',')  # Assuming comma deliminated values
            data = self.clean_column_names(data)
            self.logger.info(f"Successfully read file: {filepath}")
        except FileNotFoundError:
            self.logger.error(f"File not found: {filepath}")
            raise
        except pd.errors.EmptyDataError:
            self.logger.error("No data: The file is empty.")
            raise
        except Exception as e:
            self.logger.error(f"Error reading the file: {e}")
            raise

        data_types: Dict[str, str] = {}

        if use_parallel:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = {executor.submit(self.process_column, col, data): col for col in data.columns}
                for future in concurrent.futures.as_completed(futures):
                    col, dtype, converted_series = future.result()
                    data_types[col] = dtype
                    data[col] = converted_series
        else:
            for col in data.columns:
                col_name, dtype, converted_series = self.process_column(col, data)
                data_types[col_name] = dtype
                data[col_name] = converted_series

        # Generate type conversion report
        report = pd.DataFrame(list(data_types.items()), columns=['Column', 'Type'])
        report.to_csv(report_path, index=False)
        self.logger.info(f"Type conversion report saved to {report_path}")

        return data

    def get_category_mapping(self) -> Dict[str, Dict[int, str]]:
        """
        Get the mapping of categorical codes to original categories.

        Returns:
            Dict[str, Dict[int, str]]: Mapping for each categorical column.
        """
        return self.series_mapping

```

"Process_Data.py"

```
# Process_Data.py

import pandas as pd
import os
from Binning_Tab.Detect_Dtypes import DtypeDetector  # Ensure dtype_detector.py is in the same directory or PYTHONPATH
import logging
from typing import Optional, Dict, Any, Tuple

class DataProcessor:
    def __init__(
        self,
        input_filepath: str = 'Data.csv',
        output_filepath: str = 'Processed_Data.csv',
        report_path: str = 'Type_Conversion_Report.csv',
        return_category_mappings: bool = True,
        mapping_directory: str = 'Category_Mappings',
        parallel_processing: bool = False,
        date_threshold: float = 0.6,
        numeric_threshold: float = 0.9,
        factor_threshold_ratio: float = 0.2,
        factor_threshold_unique: int = 500,
        dayfirst: bool = True,
        log_level: str = 'INFO',
        log_file: Optional[str] = None,
        convert_factors_to_int: bool = True,
        date_format: Optional[str] = '%d-%m-%Y',  # **New Parameter**
        save_type: str = 'csv' # **New Parameter**
    ):
        """
        Initialize the DataProcessor with file paths and configuration parameters.

        Parameters:
            ... [Existing Parameters] ...
            date_format (Optional[str]): Desired date format for output (e.g., '%d-%m-%Y'). 
                If specified, date columns will be formatted as strings in this format.
        """
        # Assigning file paths and configurations
        self.input_filepath = input_filepath
        self.output_filepath = output_filepath
        self.report_path = report_path
        self.return_category_mappings = return_category_mappings
        self.mapping_directory = mapping_directory
        self.parallel_processing = parallel_processing
        self.save_type = save_type

        # Initialize the DtypeDetector with provided thresholds and configurations
        self.detector = DtypeDetector(
            date_threshold=date_threshold,
            numeric_threshold=numeric_threshold,
            factor_threshold_ratio=factor_threshold_ratio,
            factor_threshold_unique=factor_threshold_unique,
            dayfirst=dayfirst,
            log_level=log_level,
            log_file=log_file,
            convert_factors_to_int=convert_factors_to_int,
            date_format=date_format  # **Pass the New Parameter**
        )

    def process(self):
        """
        Execute the data processing workflow:
            1. Read and clean the input CSV file.
            2. Determine and convert column data types.
            3. Generate and save a type conversion report.
            4. Save the processed data to the output CSV file.
            5. Optionally, save category mappings for 'factor' columns.
        """
        try:
            # Attempt to process the dataframe with the specified parallel processing option
            processed_data = self.detector.process_dataframe(
                filepath=self.input_filepath,
                use_parallel=self.parallel_processing,
                report_path=self.report_path
            )
            print(processed_data.dtypes)
        except Exception as e:
            # If parallel processing fails, attempt sequential processing
            print("Unable to use parallel processing, trying without parallel processing.")
            self.detector.logger.warning(f"Parallel processing failed: {e}")
            try:
                processed_data = self.detector.process_dataframe(
                    filepath=self.input_filepath,
                    use_parallel=False,
                    report_path=self.report_path
                )
            except Exception as e:
                # If sequential processing also fails, log the error and exit
                print(f"Error processing data: {e}")
                self.detector.logger.error(f"Error processing data: {e}")
                return


        if self.save_type == 'csv':
            #---------------------------------------------------------------------------
            # Save the processed data to the specified output CSV file
            try:
                processed_data.to_csv(self.output_filepath, index=False)
                self.detector.logger.info(f"Processed data saved to {self.output_filepath}")
            except Exception as e:
                self.detector.logger.error(f"Failed to save processed data: {e}")
                print(f"Failed to save processed data: {e}")
            #---------------------------------------------------------------------------

        elif self.save_type == 'pickle':
            #---------------------------------------------------------------------------
            # Save the processed data to the specified output Pickle file
            try:
                processed_data.to_pickle(self.output_filepath)
                self.detector.logger.info(f"Processed data saved to {self.output_filepath}")
            except Exception as e:
                self.detector.logger.error(f"Failed to save processed data: {e}")
                print(f"Failed to save processed data: {e}")
            #---------------------------------------------------------------------------

        # If configured to return category mappings, save them
        if self.return_category_mappings:
            self.save_category_mappings()

        return processed_data
        

    def save_category_mappings(self):
        """
        Save category mappings for 'factor' columns to the specified directory.
        Each mapping is saved as a separate CSV file named '<Column_Name>_mapping.csv'.
        """
        try:
            # Create the mapping directory if it doesn't exist
            os.makedirs(self.mapping_directory, exist_ok=True)
            self.detector.logger.debug(f"Mapping directory '{self.mapping_directory}' is ready.")
        except Exception as e:
            self.detector.logger.error(f"Failed to create mapping directory '{self.mapping_directory}': {e}")
            print(f"Failed to create mapping directory '{self.mapping_directory}': {e}")
            return

        # Remove existing mapping files to avoid duplication or outdated mappings
        try:
            for file in os.listdir(self.mapping_directory):
                file_path = os.path.join(self.mapping_directory, file)
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    self.detector.logger.debug(f"Removed existing mapping file '{file_path}'.")
        except Exception as e:
            self.detector.logger.error(f"Failed to clean mapping directory '{self.mapping_directory}': {e}")
            print(f"Failed to clean mapping directory '{self.mapping_directory}': {e}")
            return

        # Retrieve category mappings from the DtypeDetector
        category_mappings = self.detector.get_category_mapping()

        # Save each category mapping to a separate CSV file
        if category_mappings:
            for col, mapping in category_mappings.items():
                try:
                    # Convert the mapping dictionary to a DataFrame for easier CSV export
                    mapping_df = pd.DataFrame(list(mapping.items()), columns=['Code', 'Category'])
                    # Define the mapping file path
                    mapping_filepath = os.path.join(self.mapping_directory, f"{col}_mapping.csv")

                    # Save the mapping DataFrame to CSV
                    mapping_df.to_csv(mapping_filepath, index=False)
                    self.detector.logger.info(f"Category mapping for '{col}' saved to '{mapping_filepath}'")
                except Exception as e:
                    self.detector.logger.error(f"Failed to save category mapping for '{col}': {e}")
                    print(f"Failed to save category mapping for '{col}': {e}")
        else:
            self.detector.logger.info("No category mappings to save.")

```

"unique_bin_identifier.py"

```
# unique_bin_identifier.py

import pandas as pd
from itertools import combinations
from typing import Tuple, List, Dict, Optional
import warnings

class UniqueBinIdentifier:
    """
    A class to identify unique observations in the original DataFrame based on combinations
    of bin columns from the binned DataFrame.

    Attributes:
        original_df (pd.DataFrame): The original DataFrame with full data.
        binned_df (pd.DataFrame): The binned DataFrame with reduced bin counts.
        results (pd.DataFrame): DataFrame containing combinations and unique identification counts.
    """

    def __init__(self, original_df: pd.DataFrame, binned_df: pd.DataFrame):
        """
        Initializes the UniqueBinIdentifier with original and binned DataFrames.

        Parameters:
            original_df (pd.DataFrame): The original DataFrame with full data.
            binned_df (pd.DataFrame): The binned DataFrame with reduced bin counts.
        """
        self.original_df = original_df.reset_index(drop=True)
        self.binned_df = binned_df.reset_index(drop=True)
        self.results = pd.DataFrame()

        self._validate_dataframes()

    def _validate_dataframes(self):
        """
        Validates that the original and binned DataFrames have the same number of rows.
        """
        if len(self.original_df) != len(self.binned_df):
            raise ValueError("Original and binned DataFrames must have the same number of rows.")

    def find_unique_identifications(
        self,
        min_comb_size: int = 1,
        max_comb_size: Optional[int] = None,
        columns: Optional[List[str]] = None,
        progress_callback: Optional[callable] = None
    ) -> pd.DataFrame:
        if columns is None:
            columns = list(self.binned_df.columns)
        else:
            # Validate that provided columns exist in the binned DataFrame
            missing_cols = set(columns) - set(self.binned_df.columns)
            if missing_cols:
                raise ValueError(f"The following columns are not in the binned DataFrame: {missing_cols}")

        if max_comb_size is None:
            max_comb_size = len(columns)
        else:
            max_comb_size = min(max_comb_size, len(columns))

        if min_comb_size < 1:
            raise ValueError("min_comb_size must be at least 1.")

        if max_comb_size < min_comb_size:
            raise ValueError("max_comb_size must be greater than or equal to min_comb_size.")

        results = []

        total_combinations = sum(
            [self._nCr(len(columns), r) for r in range(min_comb_size, max_comb_size + 1)]
        )

        print(f"Total combinations to analyze: {total_combinations}")

        combination_counter = 0

        for comb_size in range(min_comb_size, max_comb_size + 1):
            for comb in combinations(columns, comb_size):
                combination_counter += 1
                if progress_callback and combination_counter % 1000 == 0:
                    progress_callback(combination_counter, total_combinations)
                # Create a temporary DataFrame with the combination of bins
                temp_df = self.binned_df[list(comb)]

                # Group by the combination and count the number of occurrences
                group_counts = temp_df.groupby(list(comb)).size()

                # Number of unique identifications is the number of groups with size ==1
                unique_identifications = (group_counts == 1).sum()

                # Append the result
                results.append({
                    'Combination': comb,
                    'Unique_Identifications': unique_identifications
                })

        # Create a DataFrame from the results
        self.results = pd.DataFrame(results)

        # Sort the results by 'Unique_Identifications' descending
        self.results.sort_values(by='Unique_Identifications', ascending=False, inplace=True)
        self.results.reset_index(drop=True, inplace=True)

        print("Unique identification analysis complete.")

        return self.results

    @staticmethod
    def _nCr(n: int, r: int) -> int:
        """
        Computes the number of combinations (n choose r).

        Parameters:
            n (int): Total number of items.
            r (int): Number of items to choose.

        Returns:
            int: Number of combinations.
        """
        from math import comb
        return comb(n, r)

    def get_results(self) -> pd.DataFrame:
        """
        Retrieves the results of the unique identification analysis.

        Returns:
            pd.DataFrame: DataFrame with columns 'Combination' and 'Unique_Identifications'.
        """
        if self.results.empty:
            raise ValueError("No results found. Please run 'find_unique_identifications' first.")
        return self.results.copy()

    def save_results(self, filepath: str):
        """
        Saves the unique identification results to a CSV file.

        Parameters:
            filepath (str): The path where the results will be saved.
        """
        if self.results.empty:
            raise ValueError("No results to save. Please run 'find_unique_identifications' first.")
        self.results.to_csv(filepath, index=False)
        print(f"📄 Unique identification results saved to {filepath}")

    def plot_results(self, top_n: int = 10, save_path: Optional[str] = None):
        """
        Plots the top N combinations with the highest number of unique identifications.

        Parameters:
            top_n (int, optional): Number of top combinations to plot. Default is 10.
            save_path (str, optional): If provided, saves the plot to the specified path.
        """
        if self.results.empty:
            raise ValueError("No results to plot. Please run 'find_unique_identifications' first.")

        import matplotlib.pyplot as plt
        import matplotlib.ticker as ticker

        # Select top N combinations
        plot_data = self.results.head(top_n)

        # Prepare labels
        labels = [' & '.join(comb) for comb in plot_data['Combination']]
        counts = plot_data['Unique_Identifications']

        # Create the bar plot
        fig, ax = plt.subplots(figsize=(12, 8))
        bars = ax.barh(labels, counts, color='skyblue')
        ax.set_xlabel('Number of Unique Identifications')
        ax.set_title(f'Top {top_n} Bin Combinations for Unique Identifications')
        ax.invert_yaxis()  # Highest at the top

        # Add counts to the bars
        for bar in bars:
            width = bar.get_width()
            ax.annotate(f'{width}',
                        xy=(width, bar.get_y() + bar.get_height() / 2),
                        xytext=(5, 0),  # 5 points horizontal offset
                        textcoords="offset points",
                        ha='left', va='center')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, bbox_inches='tight')
            print(f"📈 Plot saved to {save_path}")
        else:
            plt.show()

```

"utils.py"

```
# utils.py

import streamlit as st
import pandas as pd
import tempfile
import os
from Binning_Tab.Process_Data import DataProcessor
import matplotlib.pyplot as plt
import traceback
from Binning_Tab.density_plotter import DensityPlotter
from Binning_Tab.data_integrity_assessor import DataIntegrityAssessor
from Binning_Tab.unique_bin_identifier import UniqueBinIdentifier

# Define the root output directory
OUTPUT_DIR = "outputs"

# Define subdirectories
PROCESSED_DATA_DIR = os.path.join(OUTPUT_DIR, "processed_data")
REPORTS_DIR = os.path.join(OUTPUT_DIR, "reports")
PLOTS_DIR = os.path.join(OUTPUT_DIR, "plots")
UNIQUE_IDENTIFICATIONS_DIR = os.path.join(OUTPUT_DIR, "unique_identifications")
CAT_MAPPING_DIR = os.path.join(OUTPUT_DIR, "category_mappings")

def create_output_directories():
    """
    Creates the necessary output directories if they don't exist.
    """
    directories = [
        PROCESSED_DATA_DIR,
        REPORTS_DIR,
        PLOTS_DIR,
        UNIQUE_IDENTIFICATIONS_DIR
    ]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

def hide_streamlit_style():
    """
    Hides Streamlit's default menu and footer for a cleaner interface.
    """
    hide_style = """
        <style>
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        </style>
        """
    st.markdown(hide_style, unsafe_allow_html=True)

def run_processing(save_type='csv', output_filename='Processed_Data.csv', file_path='Data.csv'):
    """
    Initializes and runs the data processor, saving outputs to the designated directories.
    """
    try:
        # Ensure output directories exist
        create_output_directories()
        
        # Define output file paths
        output_filepath = os.path.join(PROCESSED_DATA_DIR, output_filename)
        report_path = os.path.join(REPORTS_DIR, 'Type_Conversion_Report.csv')
        
        processor = DataProcessor(
            input_filepath=file_path,
            output_filepath=output_filepath,
            report_path=report_path,
            return_category_mappings=True,
            mapping_directory=CAT_MAPPING_DIR,
            parallel_processing=False,
            date_threshold=0.6,
            numeric_threshold=0.9,
            factor_threshold_ratio=0.2,
            factor_threshold_unique=1000,
            dayfirst=True,
            log_level='INFO',
            log_file=None,
            convert_factors_to_int=True,
            date_format=None,  # Keep as None to retain datetime dtype
            save_type=save_type
        )
        processed_data = processor.process()
        return processed_data
        
    except Exception as e:
        st.error(f"Error during data processing: {e}")
        st.stop()

def load_data(file_type, uploaded_file):
    """
    Loads the uploaded file into a Pandas DataFrame without any processing.

    Parameters:
        file_type (str): Type of the uploaded file ('csv' or 'pkl').
        uploaded_file (UploadedFile): The uploaded file object.

    Returns:
        pd.DataFrame: The loaded DataFrame.
        str: Error message if any, else None.
    """
    if uploaded_file is None:
        return None, "No file uploaded!"

    try:
        # Determine the appropriate file extension
        file_extension = {
            "pkl": "pkl",
            "csv": "csv"
        }.get(file_type, "csv")  # Default to 'csv' if type is unrecognized

        # Create a temporary file with the correct extension
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as tmp_file:
            tmp_file.write(uploaded_file.getbuffer())
            temp_file_path = tmp_file.name

        # Read the data into a DataFrame
        if file_type == "pkl":
            Data = pd.read_pickle(temp_file_path)
        elif file_type == "csv":
            Data = pd.read_csv(temp_file_path)
        else:
            return None, "Unsupported file type!"

        # Clean up temporary file
        os.remove(temp_file_path)

        return Data, None
    except Exception as e:
        return None, f"Error loading data: {e}"

def align_dataframes(original_df, binned_df):
    """
    Ensures both DataFrames have the same columns.
    """
    try:
        # Identify columns that exist in the original DataFrame but not in the binned DataFrame
        missing_in_binned = original_df.columns.difference(binned_df.columns)
        
        # Retain all original columns that were not binned in the binned DataFrame
        for column in missing_in_binned:
            binned_df[column] = original_df[column]
        
        # Ensure columns are ordered the same way
        binned_df = binned_df[original_df.columns]
        
        return original_df, binned_df
    except Exception as e:
        st.error(f"Error aligning dataframes: {e}")
        st.stop()

def save_dataframe(df, file_type, filename, subdirectory):
    """
    Saves the DataFrame to the specified file type within a subdirectory.
    """
    try:
        # Ensure output directories exist
        create_output_directories()
        
        # Determine the full path
        if subdirectory == "processed_data":
            dir_path = PROCESSED_DATA_DIR
        elif subdirectory == "reports":
            dir_path = REPORTS_DIR
        elif subdirectory == "unique_identifications":
            dir_path = UNIQUE_IDENTIFICATIONS_DIR
        elif subdirectory == "plots":
            dir_path = PLOTS_DIR
        else:
            raise ValueError("Unsupported subdirectory for saving DataFrame.")

        file_path = os.path.join(dir_path, filename)

        if file_type == 'csv':
            df.to_csv(file_path, index=False)
        elif file_type == 'pkl':
            df.to_pickle(file_path)
        else:
            raise ValueError("Unsupported file type for saving.")
        
        return file_path  # Return the path for further use if needed
    except Exception as e:
        st.error(f"Error saving DataFrame: {e}")
        st.stop()

def load_dataframe(file_path, file_type):
    """
    Loads a DataFrame from the specified file path and type.
    """
    try:
        if file_type == 'csv':
            return pd.read_csv(file_path)
        elif file_type == 'pkl':
            return pd.read_pickle(file_path)
        else:
            raise ValueError("Unsupported file type for loading.")
    except Exception as e:
        st.error(f"Error loading DataFrame: {e}")
        st.stop()

def get_binning_configuration(Data, selected_columns):
    """
    Generates binning configuration sliders for selected columns.

    Args:
        Data (pd.DataFrame): The DataFrame containing the data.
        selected_columns (list): List of columns selected for binning.

    Returns:
        dict: A dictionary with column names as keys and number of bins as values.
    """
    bins = {}
    st.markdown("### 📏 Binning Configuration")

    # Define layout parameters
    cols_per_row = 2  # Number of sliders per row
    num_cols = len(selected_columns)
    rows = num_cols // cols_per_row + (num_cols % cols_per_row > 0)

    current_col = 0

    for row in range(rows):
        # Create a new row of columns
        cols = st.columns(cols_per_row)
        
        for col_idx in range(cols_per_row):
            if current_col < num_cols:
                column = selected_columns[current_col]
                max_bins = Data[column].nunique()
                min_bins = 2 if max_bins >= 2 else 1  # At least 2 bins if possible
                default_bins = min(10, max_bins) if max_bins >= 2 else 1  # Default to 10 or max_unique if lower

                with cols[col_idx]:
                    if pd.api.types.is_datetime64_any_dtype(Data[column]):
                        default_bins = min(6, max_bins) if max_bins >= 2 else 1
                        bins[column] = st.slider(
                            f'📏 {column} (Datetime)', 
                            min_value=1, 
                            max_value=max_bins,
                            value=default_bins,
                            key=column
                        )
                    elif pd.api.types.is_integer_dtype(Data[column]):
                        if max_bins > 2:
                            bins[column] = st.slider(
                                f'📏 {column} (Integer)', 
                                min_value=2, 
                                max_value=max_bins, 
                                value=default_bins,
                                key=column
                            )
                        elif max_bins == 2:
                            st.write(f'📏 **{column} (Integer):** 2 (Fixed)')
                            bins[column] = 2
                        else:
                            st.write(f'📏 **{column} (Integer):** {max_bins} (Fixed)')
                            bins[column] = max_bins
                    elif pd.api.types.is_float_dtype(Data[column]):
                        if max_bins > 2:
                            bins[column] = st.slider(
                                f'📏 {column} (Float)', 
                                min_value=2, 
                                max_value=max_bins, 
                                value=default_bins,
                                key=column
                            )
                        elif max_bins == 2:
                            st.write(f'📏 **{column} (Float):** 2 (Fixed)')
                            bins[column] = 2
                        else:
                            st.write(f'📏 **{column} (Float):** {max_bins} (Fixed)')
                            bins[column] = max_bins
                    else:
                        if max_bins > 1:
                            bins[column] = st.slider(
                                f'📏 {column}', 
                                min_value=1, 
                                max_value=max_bins, 
                                value=default_bins,
                                key=column
                            )
                        elif max_bins == 1:
                            st.write(f'📏 **{column}:** 1 (Fixed)')
                            bins[column] = 1
                        else:
                            st.write(f'📏 **{column}:** {max_bins} (Fixed)')
                            bins[column] = max_bins

                current_col += 1

    return bins

def plot_entropy_and_display(assessor, plots_dir):
    """
    Plots the entropy and displays it in Streamlit.

    Args:
        assessor (DataIntegrityAssessor): The integrity assessor instance.
        plots_dir (str): Directory to save the entropy plot.
    """
    st.markdown("### 📈 Entropy")
    try:
        fig_entropy = assessor.plot_entropy(figsize=(15, 4))  # Create the entropy plot
        # Save the entropy plot
        entropy_plot_path = os.path.join(plots_dir, 'entropy_plot.png')
        fig_entropy.savefig(entropy_plot_path, bbox_inches='tight')
        # Display in Streamlit before closing
        st.pyplot(fig_entropy)  # to display
        plt.close(fig_entropy)  # Close the figure to free memory
    except Exception as e:
        st.error(f"Error plotting entropy: {e}")
        st.error(traceback.format_exc())  # Detailed error log

def plot_density_plots_and_display(original_df, binned_df, selected_columns, plots_dir):
    """
    Plots the density plots for original and binned data and displays them in Streamlit.

    Args:
        original_df (pd.DataFrame): Original DataFrame for assessment.
        binned_df (pd.DataFrame): Binned DataFrame for assessment.
        selected_columns (list): Columns selected for binning.
        plots_dir (str): Directory to save the density plots.
    """
    st.markdown("### 📈 Density Plots")
    if len(selected_columns) > 1:
        density_tab1, density_tab2 = st.tabs(["Original Data", "Binned Data"])
        
        with density_tab1:
            try:
                plotter_orig = DensityPlotter(
                    dataframe=original_df,  # Use original categorical data
                    category_columns=selected_columns,
                    figsize=(15, 4),                     
                    save_path=None,  # We'll handle saving manually
                    plot_style='ticks'
                )
                
                fig_orig = plotter_orig.plot_grid()
                # Save the plot
                original_density_plot_path = os.path.join(plots_dir, 'original_density_plots.png')
                fig_orig.savefig(original_density_plot_path, bbox_inches='tight')
                # Display before closing
                st.pyplot(fig_orig)
                plt.close(fig_orig)
            except Exception as e:
                st.error(f"Error plotting original data density: {e}")
                st.error(traceback.format_exc())  # Detailed error log

        with density_tab2:
            try:
                plotter_binned = DensityPlotter(
                    dataframe=binned_df,  # Use user-specified binned data
                    category_columns=selected_columns,
                    figsize=(15, 4),                     
                    save_path=None,  # We'll handle saving manually
                    plot_style='ticks'
                )
                fig_binned = plotter_binned.plot_grid()
                # Save the plot
                binned_density_plot_path = os.path.join(plots_dir, 'binned_density_plots.png')
                fig_binned.savefig(binned_density_plot_path, bbox_inches='tight')
                # Display before closing
                st.pyplot(fig_binned)
                plt.close(fig_binned)
            except Exception as e:
                st.error(f"Error plotting binned data density: {e}")
                st.error(traceback.format_exc())  # Detailed error log
    else:
        # Print a message if only one column is selected
        st.info("🔄 **Please select more than one column to display density plots.**")

def handle_download_binned_data(data, file_type_download, save_dataframe_func, plots_dir):
    """
    Handles the download functionality for binned data.

    Args:
        data (pd.DataFrame): The binned DataFrame to be downloaded.
        file_type_download (str): The selected file type for download ('csv' or 'pkl').
        save_dataframe_func (callable): Function to save the DataFrame.
        plots_dir (str): Directory to save any related plots if necessary.
    """
    st.markdown("### 💾 Download Binned Data")
    try:
        if file_type_download == 'csv':
            binned_csv = data.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="📥 Download Binned Data as CSV",
                data=binned_csv,
                file_name='binned_data.csv',
                mime='text/csv',
            )
        elif file_type_download == 'pkl':
            # Save pickle to the 'processed_data' directory
            pickle_filename = 'binned_data.pkl'
            pickle_path = save_dataframe_func(data, 'pkl', pickle_filename, 'processed_data')
            
            with open(pickle_path, 'rb') as f:
                binned_pkl = f.read()
            
            st.download_button(
                label="📥 Download Binned Data as Pickle",
                data=binned_pkl,
                file_name='binned_data.pkl',
                mime='application/octet-stream',
            )
    except Exception as e:
        st.error(f"Error during data download: {e}")
        st.error(traceback.format_exc())  # Detailed error log

def handle_integrity_assessment(original_df, binned_df, plots_dir):
    """
    Handles the integrity assessment process, including generating reports and plotting entropy.

    Args:
        original_df (pd.DataFrame): Original DataFrame for assessment.
        binned_df (pd.DataFrame): Binned DataFrame for assessment.
        plots_dir (str): Directory to save plots.

    Returns:
        None
    """
    st.markdown("### 📄 Integrity Loss Report")
    try:
        assessor = DataIntegrityAssessor(original_df=original_df, binned_df=binned_df)
        assessor.assess_integrity_loss()
        report = assessor.generate_report()
        
        # Save the report to the 'reports' directory
        report_filename = 'Integrity_Loss_Report.csv'
        report_path = save_dataframe(report, 'csv', report_filename, 'reports')
        
        st.dataframe(report)
        
        overall_loss = assessor.get_overall_loss()
        st.write(f"📊 **Overall Average Integrity Loss:** {overall_loss:.2f}%")
        
        # Plot and display entropy using utility function
        plot_entropy_and_display(assessor, plots_dir)
    except Exception as e:
        st.error(f"Error during integrity assessment: {e}")
        st.error(traceback.format_exc())  # Detailed error log

def handle_unique_identification_analysis(original_df, binned_df, bin_columns_list, min_comb_size, max_comb_size):
    """
    Handles the unique identification analysis process, including progress updates, result display, and downloads.

    Args:
        original_df (pd.DataFrame): Original DataFrame for analysis.
        binned_df (pd.DataFrame): Binned DataFrame for analysis.
        bin_columns_list (list): List of bin columns to consider.
        min_comb_size (int): Minimum combination size.
        max_comb_size (int): Maximum combination size.

    Returns:
        pd.DataFrame: Results of the unique identification analysis.
    """
    try:
        with st.spinner('🔍 Analyzing unique identifications... This may take a while for large datasets.'):
            progress_bar = st.progress(0)
            # Calculate total combinations for progress tracking
            from math import comb
            total_combinations = sum(comb(len(bin_columns_list), r) for r in range(min_comb_size, max_comb_size + 1))

            def update_progress(combination_counter, total_combinations):
                progress = combination_counter / total_combinations
                st.session_state.progress = min(progress, 1.0)
                progress_bar.progress(st.session_state.progress)
            
            # Initialize the UniqueBinIdentifier
            identifier = UniqueBinIdentifier(original_df=original_df, binned_df=binned_df)                    
            # Perform unique identification analysis
            results = identifier.find_unique_identifications(
                min_comb_size=min_comb_size, 
                max_comb_size=max_comb_size, 
                columns=bin_columns_list,
                progress_callback=update_progress
            )
            progress_bar.empty()
            
            return results
    except Exception as e:
        st.error(f"Error during unique identification analysis: {e}")
        st.error(traceback.format_exc())  # Detailed error log
        return None

def display_unique_identification_results(results):
    """
    Displays the unique identification analysis results and provides download options.

    Args:
        results (pd.DataFrame): Results of the unique identification analysis.

    Returns:
        None
    """
    if results is not None:
        # Display the results
        st.success("✅ Unique Identification Analysis Completed!")
        st.write("📄 **Unique Identification Results:**")
        st.dataframe(results) 
        
        # Save the results to 'unique_identifications' directory
        unique_id_filename = 'unique_identifications.csv'
        unique_id_path = save_dataframe(results, 'csv', unique_id_filename, 'unique_identifications')
        
        # Allow user to download the results
        csv = results.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="📥 Download Results as CSV",
            data=csv,
            file_name='unique_identifications.csv',
            mime='text/csv',
        )

```

